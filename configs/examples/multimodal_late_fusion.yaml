# ============================================================================
# Multimodal Late Fusion Pipeline
# ============================================================================
# This config trains a MultimodalLateFusion model that combines predictions
# from a pre-trained aerial model (UNetFormer) and a pre-trained Sentinel-2
# temporal model (TSViT) through learned fusion weights.
#
# PREREQUISITES:
#   1. Train an aerial model and save the checkpoint (best_model.pt)
#   2. Train a Sentinel model and save the checkpoint (best_model.pt)
#   3. Set the checkpoint paths below in model.aerial_checkpoint and
#      model.sentinel_checkpoint
#
# Fusion modes:
#   - "weighted"  — Per-class learnable weights (softmax-normalized)
#   - "gated"     — Content-aware spatial gating network
#   - "concat"    — Concatenation + 1x1 convolution
#   - "average"   — Simple averaging of logits
#
# Usage:
#   python -m src.pipeline.multimodal_pipeline \
#       -c configs/examples/multimodal_late_fusion.yaml
# ============================================================================

data:
  train:
    images:   ./data/flair_2_aerial_train
    masks:    ./data/flair_2_labels_train
    sentinel: ./data/flair_2_sen_train
  val:
    images:   ./data/flair_2_aerial_test
    masks:    ./data/flair_2_labels_test
    sentinel: ./data/flair_2_sen_test
  test:
    images:   ./data/flair_2_aerial_test
    masks:    ./data/flair_2_labels_test
    sentinel: ./data/flair_2_sen_test

  centroids_path: ./data/flair-2_centroids_sp_to_patch.json
  dataset_version: multimodal_v1
  num_classes: 13
  batch_size: 4
  num_workers: 4
  pin_memory: true

  # Aerial image channels (same as aerial pipeline)
  selected_channels:
    - 0    # Red
    - 1    # Green
    - 2    # Blue
    - 3    # NIR
    - 4    # Elevation

  # Normalization for aerial images
  normalization:
    enabled: true
    mean: [0.485, 0.456, 0.406, 0.409878, 0.068752]
    std:  [0.229, 0.224, 0.225, 0.155107, 0.118672]
    scale_to_unit: [true, true, true, true, false]
    elevation_range: [0.0, 255.0]
    elevation_channel_index: 4

  # Sentinel-2 options
  sentinel_patch_size: 10
  use_monthly_average: true
  cloud_snow_cover_threshold: 0.6
  cloud_snow_prob_threshold: 50
  sentinel_scale_factor: 10000.0     # Sentinel-2 reflectance scaling factor

  other_class_index: 12

mlflow:
  name: multimodal-fusion
  run_name: late_fusion_weighted
  tracking_uri: null
  note: "Late fusion: UNetFormer (aerial) + TSViT (Sentinel-2)"
  dagshub:
    enabled: false
    repo_owner: your-username
    repo_name: flair-2
    branch: null

# ---------------------------------------------------------------------------
# Model Configuration — Multimodal Late Fusion
# ---------------------------------------------------------------------------
model:
  model_type: MultimodalLateFusion

  # ---- Pre-trained checkpoint paths (REQUIRED) ----
  # Set these to the paths of your trained aerial and Sentinel models.
  aerial_checkpoint: null              # e.g., ./artifacts/unetformer_best.pt
  sentinel_checkpoint: null            # e.g., ./artifacts/tsvit_best.pt

  # ---- Fusion settings ----
  freeze_encoders: true                # Freeze both pre-trained encoders
  fusion_mode: weighted                # "weighted", "gated", "concat", or "average"
  use_cloud_uncertainty: false         # Use cloud coverage as gating signal (gated mode only)

  # ---- Aerial model (UNetFormer) sub-configuration ----
  # These must match the architecture used to train the aerial checkpoint.
  aerial_model_type: UNetFormer
  aerial_in_channels: 5
  aerial_model_config:
    backbone_name: swsl_resnet18
    decode_channels: 64
    dropout: 0.1
    window_size: 8
    use_aux_head: false

  # ---- Sentinel model (TSViT) sub-configuration ----
  # These must match the architecture used to train the Sentinel checkpoint.
  sentinel_model_type: TSVIT
  sentinel_in_channels: 10
  sentinel_model_config:
    image_size: 10
    patch_size: 2
    max_seq_len: 12                    # 12 months when using monthly averages
    dim: 128
    temporal_depth: 4
    spatial_depth: 4
    num_heads: 4
    mlp_dim: 256
    dropout: 0.1
    emb_dropout: 0.1

training:
  device: cuda
  optimizer:
    type: AdamW
    learning_rate: 0.001               # Higher LR — only training fusion weights
    betas: [0.9, 0.999]
    weight_decay: 0.0001
  lr_scheduler:
    type: ReduceLROnPlateau
    args:
      mode: min
      factor: 0.5
      patience: 5
      threshold: 0.0001
  epochs: 50
  early_stopping_patience: 10
  early_stopping_criterion: miou       # Monitor mIoU for multimodal fusion
  loss_function:
    type: CrossEntropyLoss
    args:
      ignore_index: 12

experiment:
  seed: 42
  deterministic: true

evaluation:
  log_confusion_matrix: true
  log_comparison: true
  log_sample_ids: []

visualization:
  language: en
  labels:
    ground_truth: Ground Truth
    prediction: Prediction
    predicted_class: Predicted Class
    actual_class: Actual Class
    confusion_matrix_title: Confusion Matrix
